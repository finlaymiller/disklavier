{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import mido\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rich.pretty import pprint as print\n",
    "\n",
    "from glob import glob\n",
    "from accelerate import Accelerator\n",
    "from transformers import BertConfig, AutoTokenizer\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from unidecode import unidecode\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    BertModel,\n",
    "    GPT2LMHeadModel,\n",
    "    PreTrainedModel,\n",
    "    GPT2Config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "try:\n",
    "    import torch.distributed.nn\n",
    "    from torch import distributed as dist\n",
    "\n",
    "    has_distributed = True\n",
    "except ImportError:\n",
    "    has_distributed = False\n",
    "\n",
    "try:\n",
    "    import horovod.torch as hvd\n",
    "except ImportError:\n",
    "    hvd = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_SPLIT = 0.01  # Fraction of training data used for evaluation\n",
    "WANDB_KEY = \"<YOUR_WANDB_KEY>\"  # Weights and Biases API key\n",
    "\n",
    "# -------------------- Configuration for M3 Training --------------------\n",
    "M3_TRAIN_FOLDERS = [\n",
    "    \"<YOUR_TRAINING_DATA_FOLDER>\"  # Directory containing training data for M3\n",
    "]\n",
    "\n",
    "M3_EVAL_FOLDERS = [\n",
    "    \"<YOUR_EVALUATION_DATA_FOLDER>\"  # Directory containing evaluation data for M3 (optional)\n",
    "]\n",
    "\n",
    "PATCH_SIZE = 64  # Size of each patch\n",
    "PATCH_LENGTH = 512  # Length of the patches\n",
    "PATCH_NUM_LAYERS = 12  # Number of layers in the encoder\n",
    "TOKEN_NUM_LAYERS = 3  # Number of layers in the decoder\n",
    "M3_HIDDEN_SIZE = 768  # Size of the hidden layer\n",
    "\n",
    "M3_NUM_EPOCH = 100  # Maximum number of epochs for training\n",
    "M3_LEARNING_RATE = 1e-4  # Learning rate for the optimizer\n",
    "M3_BATCH_SIZE = 16  # Batch size per GPU (single card) during training\n",
    "M3_MASK_RATIO = 0.45  # Ratio of masked elements during training\n",
    "M3_DETERMINISTIC = True  # Ensures deterministic results with random seeds\n",
    "M3_WANDB_LOG = True  # Enable logging to Weights and Biases\n",
    "M3_LOAD_CKPT = True  # Load model weights from a checkpoint if available\n",
    "\n",
    "M3_WEIGHTS_PATH = (\n",
    "    \"weights_m3\"\n",
    "    + \"_h_size_\"\n",
    "    + str(M3_HIDDEN_SIZE)\n",
    "    + \"_t_layers_\"\n",
    "    + str(TOKEN_NUM_LAYERS)\n",
    "    + \"_p_layers_\"\n",
    "    + str(PATCH_NUM_LAYERS)\n",
    "    + \"_p_size_\"\n",
    "    + str(PATCH_SIZE)\n",
    "    + \"_p_length_\"\n",
    "    + str(PATCH_LENGTH)\n",
    "    + \"_lr_\"\n",
    "    + str(M3_LEARNING_RATE)\n",
    "    + \"_batch_\"\n",
    "    + str(M3_BATCH_SIZE)\n",
    "    + \"_mask_\"\n",
    "    + str(M3_MASK_RATIO)\n",
    "    + \".pth\"\n",
    ")  # Path to store the model weights\n",
    "M3_LOGS_PATH = M3_WEIGHTS_PATH.replace(\"weights\", \"logs\").replace(\n",
    "    \"pth\", \"txt\"\n",
    ")  # Path to save training logs\n",
    "\n",
    "# -------------------- Configuration for CLaMP3 Training ----------------\n",
    "CLAMP3_TRAIN_JSONL = (\n",
    "    \"<YOUR_TRAINING_JSONL_FILE>\"  # Path to the JSONL file with training data for CLaMP3\n",
    ")\n",
    "CLAMP3_EVAL_JSONL = \"<YOUR_EVALUATION_JSONL_FILE>\"  # Path to the JSONL file with evaluation data for CLaMP3 (optional)\n",
    "\n",
    "CLAMP3_HIDDEN_SIZE = 768  # Size of the hidden layer\n",
    "TEXT_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"  # Name of the pre-trained text model\n",
    "MAX_TEXT_LENGTH = 128  # Maximum allowed length for text input\n",
    "\n",
    "AUDIO_HIDDEN_SIZE = 768  # Size of the hidden layer for audio features\n",
    "AUDIO_NUM_LAYERS = 12  # Number of layers in the audio encoder\n",
    "MAX_AUDIO_LENGTH = 128  # Maximum allowed length for audio input\n",
    "\n",
    "CLAMP3_NUM_EPOCH = 100  # Maximum number of epochs for training\n",
    "CLAMP3_LEARNING_RATE = 1e-5  # Learning rate for the optimizer\n",
    "CLAMP3_BATCH_SIZE = 256  # Batch size per GPU (single card) during training\n",
    "LOGIT_SCALE = 1  # Scaling factor for contrastive loss\n",
    "\n",
    "FREEZE_TEXT = False  # Freeze the weights of the text model and text projection layer\n",
    "TEXT_DROPOUT = True  # Whether to apply dropout during text processing\n",
    "CLAMP3_DETERMINISTIC = True  # Ensures deterministic results with random seeds\n",
    "CLAMP3_LOAD_M3 = True  # Load weights from the M3 model\n",
    "CLAMP3_WANDB_LOG = True  # Enable logging to Weights and Biases\n",
    "CLAMP3_LOAD_CKPT = True  # Load weights from a checkpoint if available\n",
    "SAVE_EVERY = 5  # Save model weights every SAVE_EVERY epochs\n",
    "\n",
    "CLAMP3_WEIGHTS_PATH = (\n",
    "    \"weights_clamp3_saas\"\n",
    "    + \"_h_size_\"\n",
    "    + str(CLAMP3_HIDDEN_SIZE)\n",
    "    + \"_t_model_\"\n",
    "    + TEXT_MODEL_NAME.replace(\"/\", \"_\")\n",
    "    + \"_t_length_\"\n",
    "    + str(MAX_TEXT_LENGTH)\n",
    "    + \"_a_size_\"\n",
    "    + str(AUDIO_HIDDEN_SIZE)\n",
    "    + \"_a_layers_\"\n",
    "    + str(AUDIO_NUM_LAYERS)\n",
    "    + \"_a_length_\"\n",
    "    + str(MAX_AUDIO_LENGTH)\n",
    "    + \"_s_size_\"\n",
    "    + str(M3_HIDDEN_SIZE)\n",
    "    + \"_s_layers_\"\n",
    "    + str(PATCH_NUM_LAYERS)\n",
    "    + \"_p_size_\"\n",
    "    + str(PATCH_SIZE)\n",
    "    + \"_p_length_\"\n",
    "    + str(PATCH_LENGTH)\n",
    "    + \".pth\"\n",
    ")  # Path to store CLaMP3 model weights\n",
    "CLAMP3_LOGS_PATH = CLAMP3_WEIGHTS_PATH.replace(\"weights\", \"logs\").replace(\n",
    "    \"pth\", \"txt\"\n",
    ")  # Path to save training logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        local_loss=False,\n",
    "        gather_with_grad=False,\n",
    "        cache_labels=False,\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "        use_horovod=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.local_loss = local_loss\n",
    "        self.gather_with_grad = gather_with_grad\n",
    "        self.cache_labels = cache_labels\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.use_horovod = use_horovod\n",
    "\n",
    "        # cache state\n",
    "        self.prev_num_logits = 0\n",
    "        self.labels = {}\n",
    "\n",
    "    def gather_features(\n",
    "        self,\n",
    "        image_features,\n",
    "        text_features,\n",
    "        local_loss=False,\n",
    "        gather_with_grad=False,\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "        use_horovod=False,\n",
    "    ):\n",
    "        assert (\n",
    "            has_distributed\n",
    "        ), \"torch.distributed did not import correctly, please use a PyTorch version with support.\"\n",
    "        if use_horovod:\n",
    "            assert hvd is not None, \"Please install horovod\"\n",
    "            if gather_with_grad:\n",
    "                all_image_features = hvd.allgather(image_features)\n",
    "                all_text_features = hvd.allgather(text_features)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    all_image_features = hvd.allgather(image_features)\n",
    "                    all_text_features = hvd.allgather(text_features)\n",
    "                if not local_loss:\n",
    "                    # ensure grads for local rank when all_* features don't have a gradient\n",
    "                    gathered_image_features = list(\n",
    "                        all_image_features.chunk(world_size, dim=0)\n",
    "                    )\n",
    "                    gathered_text_features = list(\n",
    "                        all_text_features.chunk(world_size, dim=0)\n",
    "                    )\n",
    "                    gathered_image_features[rank] = image_features\n",
    "                    gathered_text_features[rank] = text_features\n",
    "                    all_image_features = torch.cat(gathered_image_features, dim=0)\n",
    "                    all_text_features = torch.cat(gathered_text_features, dim=0)\n",
    "        else:\n",
    "            # We gather tensors from all gpus\n",
    "            if gather_with_grad:\n",
    "                all_image_features = torch.cat(\n",
    "                    torch.distributed.nn.all_gather(image_features), dim=0\n",
    "                )\n",
    "                all_text_features = torch.cat(\n",
    "                    torch.distributed.nn.all_gather(text_features), dim=0\n",
    "                )\n",
    "            else:\n",
    "                gathered_image_features = [\n",
    "                    torch.zeros_like(image_features) for _ in range(world_size)\n",
    "                ]\n",
    "                gathered_text_features = [\n",
    "                    torch.zeros_like(text_features) for _ in range(world_size)\n",
    "                ]\n",
    "                dist.all_gather(gathered_image_features, image_features)\n",
    "                dist.all_gather(gathered_text_features, text_features)\n",
    "                if not local_loss:\n",
    "                    # ensure grads for local rank when all_* features don't have a gradient\n",
    "                    gathered_image_features[rank] = image_features\n",
    "                    gathered_text_features[rank] = text_features\n",
    "                all_image_features = torch.cat(gathered_image_features, dim=0)\n",
    "                all_text_features = torch.cat(gathered_text_features, dim=0)\n",
    "\n",
    "        return all_image_features, all_text_features\n",
    "\n",
    "    def get_ground_truth(self, device, num_logits) -> torch.Tensor:\n",
    "        # calculated ground-truth and cache if enabled\n",
    "        if self.prev_num_logits != num_logits or device not in self.labels:\n",
    "            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n",
    "            if self.world_size > 1 and self.local_loss:\n",
    "                labels = labels + num_logits * self.rank\n",
    "            if self.cache_labels:\n",
    "                self.labels[device] = labels\n",
    "                self.prev_num_logits = num_logits\n",
    "        else:\n",
    "            labels = self.labels[device]\n",
    "        return labels\n",
    "\n",
    "    def get_logits(self, image_features, text_features, logit_scale):\n",
    "        if self.world_size > 1:\n",
    "            all_image_features, all_text_features = self.gather_features(\n",
    "                image_features,\n",
    "                text_features,\n",
    "                self.local_loss,\n",
    "                self.gather_with_grad,\n",
    "                self.rank,\n",
    "                self.world_size,\n",
    "                self.use_horovod,\n",
    "            )\n",
    "\n",
    "            if self.local_loss:\n",
    "                logits_per_image = logit_scale * image_features @ all_text_features.T\n",
    "                logits_per_text = logit_scale * text_features @ all_image_features.T\n",
    "            else:\n",
    "                logits_per_image = (\n",
    "                    logit_scale * all_image_features @ all_text_features.T\n",
    "                )\n",
    "                logits_per_text = logits_per_image.T\n",
    "        else:\n",
    "            logits_per_image = logit_scale * image_features @ text_features.T\n",
    "            logits_per_text = logit_scale * text_features @ image_features.T\n",
    "\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "    def forward(self, image_features, text_features, logit_scale, output_dict=False):\n",
    "        device = image_features.device\n",
    "        logits_per_image, logits_per_text = self.get_logits(\n",
    "            image_features, text_features, logit_scale\n",
    "        )\n",
    "\n",
    "        labels = self.get_ground_truth(device, logits_per_image.shape[0])\n",
    "\n",
    "        total_loss = (\n",
    "            F.cross_entropy(logits_per_image, labels)\n",
    "            + F.cross_entropy(logits_per_text, labels)\n",
    "        ) / 2\n",
    "\n",
    "        return {\"contrastive_loss\": total_loss} if output_dict else total_loss\n",
    "\n",
    "\n",
    "class M3Patchilizer:\n",
    "    def __init__(self):\n",
    "        self.delimiters = [\"|:\", \"::\", \":|\", \"[|\", \"||\", \"|]\", \"|\"]\n",
    "        self.regexPattern = \"(\" + \"|\".join(map(re.escape, self.delimiters)) + \")\"\n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.mask_token_id = 3\n",
    "\n",
    "    def split_bars(self, body):\n",
    "        bars = re.split(self.regexPattern, \"\".join(body))\n",
    "        bars = list(filter(None, bars))  # remove empty strings\n",
    "        if bars[0] in self.delimiters:\n",
    "            bars[1] = bars[0] + bars[1]\n",
    "            bars = bars[1:]\n",
    "        bars = [bars[i * 2] + bars[i * 2 + 1] for i in range(len(bars) // 2)]\n",
    "        return bars\n",
    "\n",
    "    def bar2patch(self, bar, patch_size=PATCH_SIZE):\n",
    "        patch = [self.bos_token_id] + [ord(c) for c in bar] + [self.eos_token_id]\n",
    "        patch = patch[:patch_size]\n",
    "        patch += [self.pad_token_id] * (patch_size - len(patch))\n",
    "        return patch\n",
    "\n",
    "    def patch2bar(self, patch):\n",
    "        return \"\".join(chr(idx) if idx > self.mask_token_id else \"\" for idx in patch)\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        item,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        add_special_patches=False,\n",
    "        truncate=False,\n",
    "        random_truncate=False,\n",
    "    ):\n",
    "        item = item.replace(\"L:1/8\\n\", \"\")\n",
    "        item = unidecode(item)\n",
    "        lines = re.findall(r\".*?\\n|.*$\", item)\n",
    "        lines = list(filter(None, lines))  # remove empty lines\n",
    "\n",
    "        patches = []\n",
    "\n",
    "        if lines[0].split(\" \")[0] == \"ticks_per_beat\":\n",
    "            patch = \"\"\n",
    "            for line in lines:\n",
    "                if patch.startswith(line.split(\" \")[0]) and (\n",
    "                    len(patch) + len(\" \".join(line.split(\" \")[1:])) <= patch_size - 2\n",
    "                ):\n",
    "                    patch = patch[:-1] + \"\\t\" + \" \".join(line.split(\" \")[1:])\n",
    "                else:\n",
    "                    if patch:\n",
    "                        patches.append(patch)\n",
    "                    patch = line\n",
    "            if patch != \"\":\n",
    "                patches.append(patch)\n",
    "        else:\n",
    "            for line in lines:\n",
    "                if len(line) > 1 and (\n",
    "                    (line[0].isalpha() and line[1] == \":\") or line.startswith(\"%%\")\n",
    "                ):\n",
    "                    patches.append(line)\n",
    "                else:\n",
    "                    bars = self.split_bars(line)\n",
    "                    if bars:\n",
    "                        bars[-1] += \"\\n\"\n",
    "                        patches.extend(bars)\n",
    "\n",
    "        if add_special_patches:\n",
    "            bos_patch = chr(self.bos_token_id) * patch_size\n",
    "            eos_patch = chr(self.eos_token_id) * patch_size\n",
    "            patches = [bos_patch] + patches + [eos_patch]\n",
    "\n",
    "        if len(patches) > PATCH_LENGTH and truncate:\n",
    "            choices = [\"head\", \"tail\", \"middle\"]\n",
    "            choice = random.choice(choices)\n",
    "            if choice == \"head\" or random_truncate == False:\n",
    "                patches = patches[:PATCH_LENGTH]\n",
    "            elif choice == \"tail\":\n",
    "                patches = patches[-PATCH_LENGTH:]\n",
    "            else:\n",
    "                start = random.randint(1, len(patches) - PATCH_LENGTH)\n",
    "                patches = patches[start : start + PATCH_LENGTH]\n",
    "\n",
    "        patches = [self.bar2patch(patch) for patch in patches]\n",
    "\n",
    "        return patches\n",
    "\n",
    "    def decode(self, patches):\n",
    "        return \"\".join(self.patch2bar(patch) for patch in patches)\n",
    "\n",
    "\n",
    "class M3PatchEncoder(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(M3PatchEncoder, self).__init__(config)\n",
    "        self.patch_embedding = torch.nn.Linear(PATCH_SIZE * 128, M3_HIDDEN_SIZE)\n",
    "        torch.nn.init.normal_(self.patch_embedding.weight, std=0.02)\n",
    "        self.base = BertModel(config=config)\n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.mask_token_id = 3\n",
    "\n",
    "    def forward(\n",
    "        self, input_patches, input_masks  # [batch_size, seq_length, hidden_size]\n",
    "    ):  # [batch_size, seq_length]\n",
    "        # Transform input_patches into embeddings\n",
    "        input_patches = torch.nn.functional.one_hot(input_patches, num_classes=128)\n",
    "        input_patches = input_patches.reshape(\n",
    "            len(input_patches), -1, PATCH_SIZE * 128\n",
    "        ).type(torch.FloatTensor)\n",
    "        input_patches = self.patch_embedding(input_patches.to(self.device))\n",
    "\n",
    "        # Apply BERT model to input_patches and input_masks\n",
    "        return self.base(inputs_embeds=input_patches, attention_mask=input_masks)\n",
    "\n",
    "\n",
    "class M3TokenDecoder(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(M3TokenDecoder, self).__init__(config)\n",
    "        self.base = GPT2LMHeadModel(config=config)\n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.mask_token_id = 3\n",
    "\n",
    "    def forward(\n",
    "        self, patch_features, target_patches  # [batch_size, hidden_size]\n",
    "    ):  # [batch_size, seq_length]\n",
    "        # get input embeddings\n",
    "        inputs_embeds = torch.nn.functional.embedding(\n",
    "            target_patches, self.base.transformer.wte.weight\n",
    "        )\n",
    "\n",
    "        # concatenate the encoded patches with the input embeddings\n",
    "        inputs_embeds = torch.cat(\n",
    "            (patch_features.unsqueeze(1), inputs_embeds[:, 1:, :]), dim=1\n",
    "        )\n",
    "\n",
    "        # preparing the labels for model training\n",
    "        target_masks = target_patches == self.pad_token_id\n",
    "        target_patches = target_patches.clone().masked_fill_(target_masks, -100)\n",
    "\n",
    "        # get the attention mask\n",
    "        target_masks = ~target_masks\n",
    "        target_masks = target_masks.type(torch.int)\n",
    "\n",
    "        return self.base(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=target_masks,\n",
    "            labels=target_patches,\n",
    "        )\n",
    "\n",
    "    def generate(self, patch_feature, tokens):\n",
    "        # reshape the patch_feature and tokens\n",
    "        patch_feature = patch_feature.reshape(1, 1, -1)\n",
    "        tokens = tokens.reshape(1, -1)\n",
    "\n",
    "        # get input embeddings\n",
    "        tokens = torch.nn.functional.embedding(tokens, self.base.transformer.wte.weight)\n",
    "\n",
    "        # concatenate the encoded patches with the input embeddings\n",
    "        tokens = torch.cat((patch_feature, tokens[:, 1:, :]), dim=1)\n",
    "\n",
    "        # get the outputs from the model\n",
    "        outputs = self.base(inputs_embeds=tokens)\n",
    "\n",
    "        # get the probabilities of the next token\n",
    "        probs = torch.nn.functional.softmax(outputs.logits.squeeze(0)[-1], dim=-1)\n",
    "\n",
    "        return probs.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "class M3Model(PreTrainedModel):\n",
    "    def __init__(self, encoder_config, decoder_config):\n",
    "        super(M3Model, self).__init__(encoder_config)\n",
    "        self.encoder = M3PatchEncoder(encoder_config)\n",
    "        self.decoder = M3TokenDecoder(decoder_config)\n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.mask_token_id = 3\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_patches,  # [batch_size, seq_length, hidden_size]\n",
    "        input_masks,  # [batch_size, seq_length]\n",
    "        selected_indices,  # [batch_size, seq_length]\n",
    "        target_patches,\n",
    "    ):  # [batch_size, seq_length, hidden_size]\n",
    "        input_patches = input_patches.reshape(len(input_patches), -1, PATCH_SIZE).to(\n",
    "            self.device\n",
    "        )\n",
    "        input_masks = input_masks.to(self.device)\n",
    "        selected_indices = selected_indices.to(self.device)\n",
    "        target_patches = target_patches.reshape(len(target_patches), -1, PATCH_SIZE).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # Pass the input_patches and input_masks through the encoder\n",
    "        outputs = self.encoder(input_patches, input_masks)[\"last_hidden_state\"]\n",
    "\n",
    "        # Use selected_indices to form target_patches\n",
    "        target_patches = target_patches[selected_indices.bool()]\n",
    "        patch_features = outputs[selected_indices.bool()]\n",
    "\n",
    "        # Pass patch_features and target_patches through the decoder\n",
    "        return self.decoder(patch_features, target_patches)\n",
    "\n",
    "\n",
    "class CLaMP3Model(PreTrainedModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_config,\n",
    "        symbolic_config,\n",
    "        global_rank=None,\n",
    "        world_size=None,\n",
    "        text_model_name=TEXT_MODEL_NAME,\n",
    "        hidden_size=CLAMP3_HIDDEN_SIZE,\n",
    "        load_m3=CLAMP3_LOAD_M3,\n",
    "    ):\n",
    "        super(CLaMP3Model, self).__init__(symbolic_config)\n",
    "\n",
    "        self.text_model = AutoModel.from_pretrained(\n",
    "            text_model_name\n",
    "        )  # Load the text model\n",
    "        self.text_proj = torch.nn.Linear(\n",
    "            self.text_model.config.hidden_size, hidden_size\n",
    "        )  # Linear layer for text projections\n",
    "        torch.nn.init.normal_(\n",
    "            self.text_proj.weight, std=0.02\n",
    "        )  # Initialize weights with normal distribution\n",
    "\n",
    "        self.symbolic_model = M3PatchEncoder(\n",
    "            symbolic_config\n",
    "        )  # Initialize the symbolic model\n",
    "        self.symbolic_proj = torch.nn.Linear(\n",
    "            M3_HIDDEN_SIZE, hidden_size\n",
    "        )  # Linear layer for symbolic projections\n",
    "        torch.nn.init.normal_(\n",
    "            self.symbolic_proj.weight, std=0.02\n",
    "        )  # Initialize weights with normal distribution\n",
    "\n",
    "        self.audio_model = BertModel(audio_config)  # Initialize the audio model\n",
    "        self.audio_proj = torch.nn.Linear(\n",
    "            audio_config.hidden_size, hidden_size\n",
    "        )  # Linear layer for audio projections\n",
    "        torch.nn.init.normal_(\n",
    "            self.audio_proj.weight, std=0.02\n",
    "        )  # Initialize weights with normal distribution\n",
    "\n",
    "        if global_rank == None or world_size == None:\n",
    "            global_rank = 0\n",
    "            world_size = 1\n",
    "\n",
    "        self.loss_fn = ClipLoss(\n",
    "            local_loss=False,\n",
    "            gather_with_grad=True,\n",
    "            cache_labels=False,\n",
    "            rank=global_rank,\n",
    "            world_size=world_size,\n",
    "            use_horovod=False,\n",
    "        )\n",
    "\n",
    "        if load_m3 and os.path.exists(M3_WEIGHTS_PATH):\n",
    "            checkpoint = torch.load(\n",
    "                M3_WEIGHTS_PATH, map_location=\"cpu\", weights_only=True\n",
    "            )\n",
    "            decoder_config = GPT2Config(\n",
    "                vocab_size=128,\n",
    "                n_positions=PATCH_SIZE,\n",
    "                n_embd=M3_HIDDEN_SIZE,\n",
    "                n_layer=TOKEN_NUM_LAYERS,\n",
    "                n_head=M3_HIDDEN_SIZE // 64,\n",
    "                n_inner=M3_HIDDEN_SIZE * 4,\n",
    "            )\n",
    "            model = M3Model(symbolic_config, decoder_config)\n",
    "            model.load_state_dict(checkpoint[\"model\"])\n",
    "            self.symbolic_model = model.encoder\n",
    "            model = None\n",
    "            print(\n",
    "                f\"Successfully Loaded M3 Checkpoint from Epoch {checkpoint['epoch']} with loss {checkpoint['min_eval_loss']}\"\n",
    "            )\n",
    "\n",
    "    def set_trainable(self, freeze_list):\n",
    "        if \"text_model\" in freeze_list:\n",
    "            self.text_model.eval()\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Text Model Frozen\")\n",
    "        else:\n",
    "            self.text_model.train()\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Text Model Training\")\n",
    "\n",
    "        if \"text_proj\" in freeze_list:\n",
    "            self.text_proj.eval()\n",
    "            for param in self.text_proj.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Text Projection Layer Frozen\")\n",
    "        else:\n",
    "            self.text_proj.train()\n",
    "            for param in self.text_proj.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Text Projection Layer Training\")\n",
    "\n",
    "        if \"symbolic_model\" in freeze_list:\n",
    "            self.symbolic_model.eval()\n",
    "            for param in self.symbolic_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Symbolic Model Frozen\")\n",
    "        else:\n",
    "            self.symbolic_model.train()\n",
    "            for param in self.symbolic_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Symbolic Model Training\")\n",
    "\n",
    "        if \"symbolic_proj\" in freeze_list:\n",
    "            self.symbolic_proj.eval()\n",
    "            for param in self.symbolic_proj.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Symbolic Projection Layer Frozen\")\n",
    "        else:\n",
    "            self.symbolic_proj.train()\n",
    "            for param in self.symbolic_proj.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Symbolic Projection Layer Training\")\n",
    "\n",
    "        if \"audio_model\" in freeze_list:\n",
    "            self.audio_model.eval()\n",
    "            for param in self.audio_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Audio Model Frozen\")\n",
    "        else:\n",
    "            self.audio_model.train()\n",
    "            for param in self.audio_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Audio Model Training\")\n",
    "\n",
    "        if \"audio_proj\" in freeze_list:\n",
    "            self.audio_proj.eval()\n",
    "            for param in self.audio_proj.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Audio Projection Layer Frozen\")\n",
    "        else:\n",
    "            self.audio_proj.train()\n",
    "            for param in self.audio_proj.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Audio Projection Layer Training\")\n",
    "\n",
    "    def avg_pooling(self, input_features, input_masks):\n",
    "        input_masks = input_masks.unsqueeze(-1).to(\n",
    "            self.device\n",
    "        )  # add a dimension to match the feature dimension\n",
    "        input_features = input_features * input_masks  # apply mask to input_features\n",
    "        avg_pool = input_features.sum(dim=1) / input_masks.sum(\n",
    "            dim=1\n",
    "        )  # calculate average pooling\n",
    "\n",
    "        return avg_pool\n",
    "\n",
    "    def get_text_features(self, text_inputs, text_masks, get_global=False):\n",
    "        text_features = self.text_model(\n",
    "            text_inputs.to(self.device), attention_mask=text_masks.to(self.device)\n",
    "        )[\"last_hidden_state\"]\n",
    "\n",
    "        if get_global:\n",
    "            text_features = self.avg_pooling(text_features, text_masks)\n",
    "            text_features = self.text_proj(text_features)\n",
    "\n",
    "        return text_features\n",
    "\n",
    "    def get_symbolic_features(self, symbolic_inputs, symbolic_masks, get_global=False):\n",
    "        symbolic_features = self.symbolic_model(\n",
    "            symbolic_inputs.to(self.device), symbolic_masks.to(self.device)\n",
    "        )[\"last_hidden_state\"]\n",
    "\n",
    "        if get_global:\n",
    "            symbolic_features = self.avg_pooling(symbolic_features, symbolic_masks)\n",
    "            symbolic_features = self.symbolic_proj(symbolic_features)\n",
    "\n",
    "        return symbolic_features\n",
    "\n",
    "    def get_audio_features(self, audio_inputs, audio_masks, get_global=False):\n",
    "        audio_features = self.audio_model(\n",
    "            inputs_embeds=audio_inputs.to(self.device),\n",
    "            attention_mask=audio_masks.to(self.device),\n",
    "        )[\"last_hidden_state\"]\n",
    "\n",
    "        if get_global:\n",
    "            audio_features = self.avg_pooling(audio_features, audio_masks)\n",
    "            audio_features = self.audio_proj(audio_features)\n",
    "\n",
    "        return audio_features\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        text_inputs,  # [batch_size, seq_length]\n",
    "        text_masks,  # [batch_size, seq_length]\n",
    "        music_inputs,  # [batch_size, seq_length, hidden_size]\n",
    "        music_masks,  # [batch_size, seq_length]\n",
    "        music_modality,\n",
    "    ):  # \"symbolic\" or \"audio\"\n",
    "        # Compute the text features\n",
    "        text_features = self.get_text_features(text_inputs, text_masks, get_global=True)\n",
    "\n",
    "        # Compute the music features\n",
    "        if music_modality == \"symbolic\":\n",
    "            music_features = self.get_symbolic_features(\n",
    "                music_inputs, music_masks, get_global=True\n",
    "            )\n",
    "        elif music_modality == \"audio\":\n",
    "            music_features = self.get_audio_features(\n",
    "                music_inputs, music_masks, get_global=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"music_modality must be either 'symbolic' or 'audio'\")\n",
    "\n",
    "        return self.loss_fn(\n",
    "            text_features, music_features, LOGIT_SCALE, output_dict=False\n",
    "        )\n",
    "\n",
    "\n",
    "def split_data(data, eval_ratio=EVAL_SPLIT):\n",
    "    random.shuffle(data)\n",
    "    split_idx = int(len(data) * eval_ratio)\n",
    "    eval_set = data[:split_idx]\n",
    "    train_set = data[split_idx:]\n",
    "    return train_set, eval_set\n",
    "\n",
    "\n",
    "def mask_patches(target_patches, patchilizer, mode):\n",
    "    indices = list(range(len(target_patches)))\n",
    "    random.shuffle(indices)\n",
    "    selected_indices = indices[: math.ceil(M3_MASK_RATIO * len(indices))]\n",
    "    sorted_indices = sorted(selected_indices)\n",
    "    input_patches = torch.tensor(target_patches)\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        choice = \"original\"\n",
    "    else:\n",
    "        choice = random.choices(\n",
    "            [\"mask\", \"shuffle\", \"original\"], weights=[0.8, 0.1, 0.1]\n",
    "        )[0]\n",
    "\n",
    "    if choice == \"mask\":\n",
    "        input_patches[sorted_indices] = torch.tensor(\n",
    "            [patchilizer.mask_token_id] * PATCH_SIZE\n",
    "        )\n",
    "    elif choice == \"shuffle\":\n",
    "        for idx in sorted_indices:\n",
    "            patch = input_patches[idx]\n",
    "            try:\n",
    "                index_eos = (patch == patchilizer.eos_token_id).nonzero().item()\n",
    "            except:\n",
    "                index_eos = len(patch)\n",
    "\n",
    "            indices = list(range(1, index_eos))\n",
    "            random.shuffle(indices)\n",
    "            indices = [0] + indices + list(range(index_eos, len(patch)))\n",
    "            input_patches[idx] = patch[indices]\n",
    "\n",
    "    selected_indices = torch.zeros(len(target_patches))\n",
    "    selected_indices[sorted_indices] = 1.0\n",
    "\n",
    "    return input_patches, selected_indices\n",
    "\n",
    "\n",
    "def remove_instrument_info(item):\n",
    "    # remove instrument information from symbolic music\n",
    "    lines = re.findall(r\".*?\\n|.*$\", item)\n",
    "    lines = list(filter(None, lines))\n",
    "    if lines[0].split(\" \")[0] == \"ticks_per_beat\":\n",
    "        type = \"mtf\"\n",
    "    else:\n",
    "        type = \"abc\"\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if type == \"abc\" and line.startswith(\"V:\"):\n",
    "            # find the position of \" nm=\" or \" snm=\"\n",
    "            nm_pos = line.find(\" nm=\")\n",
    "            snm_pos = line.find(\" snm=\")\n",
    "            # keep the part before \" nm=\" or \" snm=\"\n",
    "            if nm_pos != -1:\n",
    "                line = line[:nm_pos]\n",
    "            elif snm_pos != -1:\n",
    "                line = line[:snm_pos]\n",
    "            if nm_pos != -1 or snm_pos != -1:\n",
    "                line += \"\\n\"\n",
    "        elif type == \"mtf\" and line.startswith(\"program_change\"):\n",
    "            line = \" \".join(line.split(\" \")[:-1]) + \" 0\\n\"\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    return \"\".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basename(filename: str) -> str:\n",
    "    return os.path.splitext(os.path.basename(filename))[0]\n",
    "\n",
    "\n",
    "def msg_to_str(msg):\n",
    "    str_msg = \"\"\n",
    "    for key, value in msg.dict().items():\n",
    "        str_msg += \" \" + str(value)\n",
    "    return str_msg.strip().encode(\"unicode_escape\").decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def load_midi(filename: str, m3_compatible: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Load a MIDI file and convert it to MTF format.\n",
    "    \"\"\"\n",
    "    mid = mido.MidiFile(filename)\n",
    "    msg_list = [\"ticks_per_beat \" + str(mid.ticks_per_beat)]\n",
    "\n",
    "    # Traverse the MIDI file\n",
    "    for msg in mid.merged_track:\n",
    "        if m3_compatible:\n",
    "            if msg.is_meta:\n",
    "                if msg.type in [\n",
    "                    \"text\",\n",
    "                    \"copyright\",\n",
    "                    \"track_name\",\n",
    "                    \"instrument_name\",\n",
    "                    \"lyrics\",\n",
    "                    \"marker\",\n",
    "                    \"cue_marker\",\n",
    "                    \"device_name\",\n",
    "                ]:\n",
    "                    continue\n",
    "        str_msg = msg_to_str(msg)\n",
    "        msg_list.append(str_msg)\n",
    "\n",
    "    return \"\\n\".join(msg_list)\n",
    "\n",
    "\n",
    "def convert_midi2mtf(\n",
    "    pf_midi_in: str, pf_midi_out: str, m3_compatible: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Converts MIDI files to MTF format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = load_midi(pf_midi_in, m3_compatible)\n",
    "\n",
    "        if not output:\n",
    "            with open(\"logs/midi2mtf_error_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(pf_midi_in + \"\\n\")\n",
    "            return \"\"\n",
    "        else:\n",
    "            output_file_path = os.path.join(pf_midi_out, basename(pf_midi_in) + \".mtf\")\n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(output)\n",
    "            return output_file_path\n",
    "    except Exception as e:\n",
    "        with open(\"logs/midi2mtf_error_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(pf_midi_in + \" \" + str(e) + \"\\n\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClampModel:\n",
    "    tag = \"[#a3d2ca]clamp3[/#a3d2ca]:\"\n",
    "    name = \"CLaMP3\"\n",
    "\n",
    "    def __init__(self, device: str, epoch: int = 512):\n",
    "        print(f\"{self.tag} initializing {self.name} model\")\n",
    "\n",
    "        # Initialize accelerator and device\n",
    "        # accelerator = Accelerator()\n",
    "        # device = accelerator.device\n",
    "        self.device = device\n",
    "        print(f\"{self.tag} using device: {self.device}\")\n",
    "\n",
    "        # Model and configuration setup\n",
    "        audio_config = BertConfig(\n",
    "            vocab_size=1,\n",
    "            hidden_size=AUDIO_HIDDEN_SIZE,\n",
    "            num_hidden_layers=AUDIO_NUM_LAYERS,\n",
    "            num_attention_heads=AUDIO_HIDDEN_SIZE // 64,\n",
    "            intermediate_size=AUDIO_HIDDEN_SIZE * 4,\n",
    "            max_position_embeddings=MAX_AUDIO_LENGTH,\n",
    "        )\n",
    "        symbolic_config = BertConfig(\n",
    "            vocab_size=1,\n",
    "            hidden_size=M3_HIDDEN_SIZE,\n",
    "            num_hidden_layers=PATCH_NUM_LAYERS,\n",
    "            num_attention_heads=M3_HIDDEN_SIZE // 64,\n",
    "            intermediate_size=M3_HIDDEN_SIZE * 4,\n",
    "            max_position_embeddings=PATCH_LENGTH,\n",
    "        )\n",
    "        self.model = CLaMP3Model(\n",
    "            audio_config=audio_config,\n",
    "            symbolic_config=symbolic_config,\n",
    "            text_model_name=TEXT_MODEL_NAME,\n",
    "            hidden_size=CLAMP3_HIDDEN_SIZE,\n",
    "            load_m3=CLAMP3_LOAD_M3,\n",
    "        )\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "        self.patchilizer = M3Patchilizer()\n",
    "\n",
    "        # print parameter number\n",
    "        print(\n",
    "            \"Total Parameter Number: \"\n",
    "            + str(sum(p.numel() for p in self.model.parameters()))\n",
    "        )\n",
    "\n",
    "        # Load model weights\n",
    "        self.model.eval()\n",
    "\n",
    "        checkpoint_path = f\"/home/finlay/disklavier/data/models/weights_clamp3_c2_length_512.pth\"  # CLAMP3_WEIGHTS_PATH\n",
    "\n",
    "        if epoch is not None:\n",
    "            checkpoint_path = checkpoint_path.replace(\n",
    "                \".pth\", f\"_{epoch}.pth\"\n",
    "            )  # CLAMP3_WEIGHTS_PATH.replace(\".pth\", f\"_{epoch}.pth\")\n",
    "\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(\"No CLaMP 3 weights found. Downloading from Hugging Face...\")\n",
    "            checkpoint_url = \"https://huggingface.co/sander-wood/clamp3/resolve/main/weights_clamp3_saas_h_size_768_t_model_FacebookAI_xlm-roberta-base_t_length_128_a_size_768_a_layers_12_a_length_128_s_size_768_s_layers_12_p_size_64_p_length_512.pth\"\n",
    "            # checkpoint_path = \"/home/finlay/disklavier/data/models/weights_clamp3_saas_h_size_768_t_model_FacebookAI_xlm-roberta-base_t_length_128_a_size_768_a_layers_12_a_length_128_s_size_768_s_layers_12_p_size_64_p_length_512.pth\"\n",
    "            response = requests.get(checkpoint_url, stream=True)\n",
    "\n",
    "            response.raise_for_status()\n",
    "            total_size = int(response.headers.get(\"content-length\", 0))\n",
    "            with open(checkpoint_path, \"wb\") as f, tqdm(\n",
    "                desc=\"Downloading\",\n",
    "                total=total_size,\n",
    "                unit=\"B\",\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        bar.update(len(chunk))\n",
    "            print(\"Weights file downloaded successfully.\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n",
    "        print(\n",
    "            f\"Successfully Loaded CLaMP 3 Checkpoint from Epoch {checkpoint['epoch']} with loss {checkpoint['min_eval_loss']}\"\n",
    "        )\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        print(f\"{self.tag} model loaded successfully\")\n",
    "\n",
    "    def embed(self, filename: str, get_global: bool = True) -> torch.Tensor:\n",
    "        if filename.endswith(\".mid\"):\n",
    "            filename = convert_midi2mtf(filename, \"data\")\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            item = f.read()\n",
    "\n",
    "        if filename.endswith(\".mtf\"):\n",
    "            input_data = self.patchilizer.encode(item, add_special_patches=True)\n",
    "            input_data = torch.tensor(input_data)\n",
    "            max_input_length = PATCH_LENGTH\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file extension (must be .mtf or .mid)\")\n",
    "        input_data = self.patchilizer.encode(item, add_special_patches=True)\n",
    "        input_data = torch.tensor(input_data)\n",
    "        max_input_length = PATCH_LENGTH\n",
    "\n",
    "        segment_list = []\n",
    "        for i in range(0, len(input_data), max_input_length):\n",
    "            segment_list.append(input_data[i : i + max_input_length])\n",
    "        segment_list[-1] = input_data[-max_input_length:]\n",
    "\n",
    "        last_hidden_states_list = []\n",
    "\n",
    "        for input_segment in segment_list:\n",
    "            input_masks = torch.tensor([1] * input_segment.size(0))\n",
    "            if filename.endswith(\".mtf\"):\n",
    "                pad_indices = (\n",
    "                    torch.ones(\n",
    "                        (PATCH_LENGTH - input_segment.size(0), PATCH_SIZE)\n",
    "                    ).long()\n",
    "                    * self.patchilizer.pad_token_id\n",
    "                )\n",
    "            else:\n",
    "                pad_indices = (\n",
    "                    torch.ones(\n",
    "                        (MAX_AUDIO_LENGTH - input_segment.size(0), AUDIO_HIDDEN_SIZE)\n",
    "                    ).float()\n",
    "                    * 0.0\n",
    "                )\n",
    "            input_masks = torch.cat(\n",
    "                (input_masks, torch.zeros(max_input_length - input_segment.size(0))), 0\n",
    "            )\n",
    "            input_segment = torch.cat((input_segment, pad_indices), 0)\n",
    "\n",
    "            if filename.endswith(\".mtf\"):\n",
    "                last_hidden_states = self.model.get_symbolic_features(\n",
    "                    symbolic_inputs=input_segment.unsqueeze(0).to(self.device),\n",
    "                    symbolic_masks=input_masks.unsqueeze(0).to(self.device),\n",
    "                    get_global=get_global,\n",
    "                )\n",
    "            else:\n",
    "                last_hidden_states = self.model.get_audio_features(\n",
    "                    audio_inputs=input_segment.unsqueeze(0).to(self.device),\n",
    "                    audio_masks=input_masks.unsqueeze(0).to(self.device),\n",
    "                    get_global=get_global,\n",
    "                )\n",
    "            if not get_global:\n",
    "                last_hidden_states = last_hidden_states[\n",
    "                    :, : input_masks.sum().long().item(), :\n",
    "                ]\n",
    "            last_hidden_states_list.append(last_hidden_states)\n",
    "\n",
    "        if not get_global:\n",
    "            last_hidden_states_list = [\n",
    "                last_hidden_states[0] for last_hidden_states in last_hidden_states_list\n",
    "            ]\n",
    "            last_hidden_states_list[-1] = last_hidden_states_list[-1][\n",
    "                -(len(input_data) % max_input_length) :\n",
    "            ]\n",
    "            last_hidden_states_list = torch.concat(last_hidden_states_list, 0)\n",
    "        else:\n",
    "            full_chunk_cnt = len(input_data) // max_input_length\n",
    "            remain_chunk_len = len(input_data) % max_input_length\n",
    "            if remain_chunk_len == 0:\n",
    "                feature_weights = torch.tensor(\n",
    "                    [max_input_length] * full_chunk_cnt, device=self.device\n",
    "                ).view(-1, 1)\n",
    "            else:\n",
    "                feature_weights = torch.tensor(\n",
    "                    [max_input_length] * full_chunk_cnt + [remain_chunk_len],\n",
    "                    device=self.device,\n",
    "                ).view(-1, 1)\n",
    "\n",
    "            last_hidden_states_list = torch.concat(last_hidden_states_list, 0)\n",
    "            last_hidden_states_list = last_hidden_states_list * feature_weights\n",
    "            last_hidden_states_list = (\n",
    "                last_hidden_states_list.sum(dim=0) / feature_weights.sum()\n",
    "            )\n",
    "        return last_hidden_states_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'[#a3d2ca]clamp3[/#a3d2ca]: initializing CLaMP3 model'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m#a3d2ca\u001b[0m\u001b[32m]\u001b[0m\u001b[32mclamp3\u001b[0m\u001b[32m[\u001b[0m\u001b[32m/#a3d2ca\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: initializing CLaMP3 model'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'[#a3d2ca]clamp3[/#a3d2ca]: using device: cuda:1'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m#a3d2ca\u001b[0m\u001b[32m]\u001b[0m\u001b[32mclamp3\u001b[0m\u001b[32m[\u001b[0m\u001b[32m/#a3d2ca\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: using device: cuda:1'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Total Parameter Number: 457896960'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Total Parameter Number: 457896960'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'No CLaMP 3 weights found. Downloading from Hugging Face...'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'No CLaMP 3 weights found. Downloading from Hugging Face...'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.39G/2.39G [00:28<00:00, 91.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Weights file downloaded successfully.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Weights file downloaded successfully.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Successfully Loaded CLaMP 3 Checkpoint from Epoch 10 with loss 0.4150416124612093'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Successfully Loaded CLaMP 3 Checkpoint from Epoch 10 with loss 0.4150416124612093'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'[#a3d2ca]clamp3[/#a3d2ca]: model loaded successfully'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m#a3d2ca\u001b[0m\u001b[32m]\u001b[0m\u001b[32mclamp3\u001b[0m\u001b[32m[\u001b[0m\u001b[32m/#a3d2ca\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: model loaded successfully'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ClampModel(device=\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/datasets/20250320/segmented/20231220-080-07_0047-0053.mid'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf_dataset = os.path.join(\"..\", \"data\", \"datasets\", \"20250320\", \"segmented\")\n",
    "files = glob(os.path.join(pf_dataset, \"*.mid\"))\n",
    "random_file = random.choice(files)\n",
    "random_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'data/20231220-080-07_0047-0053.mtf'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'data/20231220-080-07_0047-0053.mtf'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ticks_per_beat 220\\nset_tempo 750000 0\\ntime_signature 4 4 24 8 0\\nprogram_change 0 0 0\\nnote_on 206 0 29 49\\nnote_on 28 0 29 0\\nnote_on 20 0 30 38\\nnote_on 32 0 30 0\\nnote_on 23 0 31 40\\nnote_on 541 0 53 44\\nnote_on 0 0 62 50\\nnote_on 2 0 58 42\\nnote_on 28 0 62 0\\nnote_on 7 0 58 0\\nnote_on 2 0 53 0\\nnote_on 106 0 53 42\\nnote_on 0 0 58 44\\nnote_on 2 0 62 49\\nnote_on 94 0 34 46\\nnote_on 4 0 31 0\\nnote_on 12 0 53 0\\nnote_on 7 0 58 0\\nnote_on 7 0 62 0\\nnote_on 80 0 34 0\\nnote_on 11 0 60 35\\nnote_on 3 0 36 34\\nnote_on 0 0 55 33\\nnote_on 4 0 64 32\\nnote_on 371 0 36 0\\nnote_on 65 0 36 63\\nnote_on 39 0 36 0\\nnote_on 11 0 34 52\\nnote_on 23 0 34 0\\nnote_on 25 0 36 49\\nnote_on 53 0 36 0\\nnote_on 7 0 34 42\\nnote_on 50 0 34 0\\nnote_on 5 0 31 44\\nnote_on 73 0 31 0\\nnote_on 321 0 55 0\\nnote_on 289 0 60 0\\nnote_on 9 0 64 0\\nend_of_track 0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtf_path = convert_midi2mtf(random_file, \"data\")\n",
    "print(mtf_path)\n",
    "with open(mtf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    item = f.read()\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = model.embed(mtf_path)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'../data/datasets/20250320/segmented/20241229-094-10_0311-0316.mid'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'../data/datasets/20250320/segmented/20241229-094-10_0311-0316.mid'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different_file = random.choice(files)\n",
    "print(different_file)\n",
    "e = model.embed(different_file)\n",
    "e.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midi-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
