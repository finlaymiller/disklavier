{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train variational autoencoder on binary MNIST data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import data\n",
    "import flow\n",
    "import argparse\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_args(parser):\n",
    "    parser.add_argument(\"--latent_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--variational\", choices=[\"flow\", \"mean-field\"])\n",
    "    parser.add_argument(\"--flow_depth\", type=int, default=2)\n",
    "    parser.add_argument(\"--data_size\", type=int, default=784)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--test_batch_size\", type=int, default=512)\n",
    "    parser.add_argument(\"--max_iterations\", type=int, default=30000)\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=10000)\n",
    "    parser.add_argument(\"--n_samples\", type=int, default=1000)\n",
    "    parser.add_argument(\"--use_gpu\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=582838)\n",
    "    parser.add_argument(\"--train_dir\", type=pathlib.Path, default=\"/tmp\")\n",
    "    parser.add_argument(\"--data_dir\", type=pathlib.Path, default=\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"Variational autoencoder, parameterized by a generative network.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size, data_size):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"p_z_loc\", torch.zeros(latent_size))\n",
    "        self.register_buffer(\"p_z_scale\", torch.ones(latent_size))\n",
    "        self.log_p_z = NormalLogProb()\n",
    "        self.log_p_x = BernoulliLogProb()\n",
    "        self.generative_network = NeuralNetwork(\n",
    "            input_size=latent_size, output_size=data_size, hidden_size=latent_size * 2\n",
    "        )\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        \"\"\"Return log probability of model.\"\"\"\n",
    "        log_p_z = self.log_p_z(self.p_z_loc, self.p_z_scale, z).sum(-1, keepdim=True)\n",
    "        logits = self.generative_network(z)\n",
    "        # unsqueeze sample dimension\n",
    "        logits, x = torch.broadcast_tensors(logits, x.unsqueeze(1))\n",
    "        log_p_x = self.log_p_x(logits, x).sum(-1, keepdim=True)\n",
    "        return log_p_z + log_p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalMeanField(nn.Module):\n",
    "    \"\"\"Approximate posterior parameterized by an inference network.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size, data_size):\n",
    "        super().__init__()\n",
    "        self.inference_network = NeuralNetwork(\n",
    "            input_size=data_size,\n",
    "            output_size=latent_size * 2,\n",
    "            hidden_size=latent_size * 2,\n",
    "        )\n",
    "        self.log_q_z = NormalLogProb()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x, n_samples=1):\n",
    "        \"\"\"Return sample of latent variable and log prob.\"\"\"\n",
    "        loc, scale_arg = torch.chunk(\n",
    "            self.inference_network(x).unsqueeze(1), chunks=2, dim=-1\n",
    "        )\n",
    "        scale = self.softplus(scale_arg)\n",
    "        eps = torch.randn((loc.shape[0], n_samples, loc.shape[-1]), device=loc.device)\n",
    "        z = loc + scale * eps  # reparameterization\n",
    "        log_q_z = self.log_q_z(loc, scale, z).sum(-1, keepdim=True)\n",
    "        return z, log_q_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalFlow(nn.Module):\n",
    "    \"\"\"Approximate posterior parameterized by a flow (https://arxiv.org/abs/1606.04934).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size, data_size, flow_depth):\n",
    "        super().__init__()\n",
    "        hidden_size = latent_size * 2\n",
    "        self.inference_network = NeuralNetwork(\n",
    "            input_size=data_size,\n",
    "            # loc, scale, and context\n",
    "            output_size=latent_size * 3,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "        modules = []\n",
    "        for _ in range(flow_depth):\n",
    "            modules.append(\n",
    "                flow.InverseAutoregressiveFlow(\n",
    "                    num_input=latent_size,\n",
    "                    num_hidden=hidden_size,\n",
    "                    num_context=latent_size,\n",
    "                )\n",
    "            )\n",
    "            modules.append(flow.Reverse(latent_size))\n",
    "        self.q_z_flow = flow.FlowSequential(*modules)\n",
    "        self.log_q_z_0 = NormalLogProb()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x, n_samples=1):\n",
    "        \"\"\"Return sample of latent variable and log prob.\"\"\"\n",
    "        loc, scale_arg, h = torch.chunk(\n",
    "            self.inference_network(x).unsqueeze(1), chunks=3, dim=-1\n",
    "        )\n",
    "        scale = self.softplus(scale_arg)\n",
    "        eps = torch.randn((loc.shape[0], n_samples, loc.shape[-1]), device=loc.device)\n",
    "        z_0 = loc + scale * eps  # reparameterization\n",
    "        log_q_z_0 = self.log_q_z_0(loc, scale, z_0)\n",
    "        z_T, log_q_z_flow = self.q_z_flow(z_0, context=h)\n",
    "        log_q_z = (log_q_z_0 + log_q_z_flow).sum(-1, keepdim=True)\n",
    "        return z_T, log_q_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        modules = [\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        ]\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalLogProb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, loc, scale, z):\n",
    "        var = torch.pow(scale, 2)\n",
    "        return -0.5 * torch.log(2 * np.pi * var) - torch.pow(z - loc, 2) / (2 * var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliLogProb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce_with_logits = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # bernoulli log prob is equivalent to negative binary cross entropy\n",
    "        return -self.bce_with_logits(logits, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(n_samples, model, variational, eval_data):\n",
    "    model.eval()\n",
    "    total_log_p_x = 0.0\n",
    "    total_elbo = 0.0\n",
    "    for batch in eval_data:\n",
    "        x = batch[0].to(next(model.parameters()).device)\n",
    "        z, log_q_z = variational(x, n_samples)\n",
    "        log_p_x_and_z = model(z, x)\n",
    "        # importance sampling of approximate marginal likelihood with q(z)\n",
    "        # as the proposal, and logsumexp in the sample dimension\n",
    "        elbo = log_p_x_and_z - log_q_z\n",
    "        log_p_x = torch.logsumexp(elbo, dim=1) - np.log(n_samples)\n",
    "        # average over sample dimension, sum over minibatch\n",
    "        total_elbo += elbo.cpu().numpy().mean(1).sum()\n",
    "        # sum over minibatch\n",
    "        total_log_p_x += log_p_x.cpu().numpy().sum()\n",
    "    n_data = len(eval_data.dataset)\n",
    "    return total_elbo / n_data, total_log_p_x / n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "parser = argparse.ArgumentParser()\n",
    "add_args(parser)\n",
    "cfg = parser.parse_args()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cfg.use_gpu else \"cpu\")\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "random.seed(cfg.seed)\n",
    "\n",
    "model = Model(latent_size=cfg.latent_size, data_size=cfg.data_size)\n",
    "if cfg.variational == \"flow\":\n",
    "    variational = VariationalFlow(\n",
    "        latent_size=cfg.latent_size,\n",
    "        data_size=cfg.data_size,\n",
    "        flow_depth=cfg.flow_depth,\n",
    "    )\n",
    "elif cfg.variational == \"mean-field\":\n",
    "    variational = VariationalMeanField(\n",
    "        latent_size=cfg.latent_size, data_size=cfg.data_size\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Variational distribution not implemented: %s\" % cfg.variational\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "variational.to(device)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(\n",
    "    list(model.parameters()) + list(variational.parameters()),\n",
    "    lr=cfg.learning_rate,\n",
    "    centered=True,\n",
    ")\n",
    "\n",
    "fname = cfg.data_dir / \"binary_mnist.h5\"\n",
    "if not fname.exists():\n",
    "    print(\"Downloading binary MNIST data...\")\n",
    "    data.download_binary_mnist(fname)\n",
    "train_data, valid_data, test_data = data.load_binary_mnist(\n",
    "    fname, cfg.batch_size, cfg.test_batch_size, cfg.use_gpu\n",
    ")\n",
    "\n",
    "best_valid_elbo = -np.inf\n",
    "num_no_improvement = 0\n",
    "train_ds = cycle(train_data)\n",
    "t0 = time.time()\n",
    "\n",
    "for step in range(cfg.max_iterations):\n",
    "    batch = next(train_ds)\n",
    "    x = batch[0].to(device)\n",
    "    model.zero_grad()\n",
    "    variational.zero_grad()\n",
    "    z, log_q_z = variational(x, n_samples=1)\n",
    "    log_p_x_and_z = model(z, x)\n",
    "    # average over sample dimension\n",
    "    elbo = (log_p_x_and_z - log_q_z).mean(1)\n",
    "    # sum over batch dimension\n",
    "    loss = -elbo.sum(0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % cfg.log_interval == 0:\n",
    "        t1 = time.time()\n",
    "        examples_per_sec = cfg.log_interval * cfg.batch_size / (t1 - t0)\n",
    "        with torch.no_grad():\n",
    "            valid_elbo, valid_log_p_x = evaluate(\n",
    "                cfg.n_samples, model, variational, valid_data\n",
    "            )\n",
    "        print(\n",
    "            f\"Step {step:<10d}\\t\"\n",
    "            f\"Train ELBO estimate: {elbo.detach().cpu().numpy().mean():<5.3f}\\t\"\n",
    "            f\"Validation ELBO estimate: {valid_elbo:<5.3f}\\t\"\n",
    "            f\"Validation log p(x) estimate: {valid_log_p_x:<5.3f}\\t\"\n",
    "            f\"Speed: {examples_per_sec:<5.2e} examples/s\"\n",
    "        )\n",
    "        if valid_elbo > best_valid_elbo:\n",
    "            num_no_improvement = 0\n",
    "            best_valid_elbo = valid_elbo\n",
    "            states = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"variational\": variational.state_dict(),\n",
    "            }\n",
    "            torch.save(states, cfg.train_dir / \"best_state_dict\")\n",
    "        t0 = t1\n",
    "\n",
    "checkpoint = torch.load(cfg.train_dir / \"best_state_dict\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "variational.load_state_dict(checkpoint[\"variational\"])\n",
    "test_elbo, test_log_p_x = evaluate(cfg.n_samples, model, variational, test_data)\n",
    "print(\n",
    "    f\"Step {step:<10d}\\t\"\n",
    "    f\"Test ELBO estimate: {test_elbo:<5.3f}\\t\"\n",
    "    f\"Test log p(x) estimate: {test_log_p_x:<5.3f}\\t\"\n",
    ")\n",
    "\n",
    "print(f\"Total time: {(time.time() - start_time) / 60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
